[master]
enable = true
# Optional: override the llama-server executable.
#
# If omitted, `check-and-run-llama-cpp.sh` prefers:
#  1) /soft/app/llama-cpp/bin/llama-server          (installed via AUTO_INFER_LLAMA_CPP_PKG_PATH)
#  2) /hard/volume/workspace/llama-cpp/build/bin/llama-server
#  3) llama-server                                 (PATH)
# llama_cpp_path = "/soft/app/llama-cpp/bin/llama-server"

[instance.control]
log_dir = "/tmp"
background = true

[instance.server]
host = "0.0.0.0"
metrics = true
# For GLM-4.7, llama.cpp/llama-server should use Jinja templates.
jinja = true
chat_template = "chatglm4"
# llama-server expects an explicit value: on|off|auto
flash_attn = "on"
# Try to fit unset args into device memory.
fit = "on"
# Recommended baseline for GLM-4.7 (adjust as needed).
ctx_size = 16384
temp = 1.0
top_p = 0.95

[instance.glm-4-7-q2k]
    [instance.glm-4-7-q2k.control]
    # Use GPUs 0, 1, 2, 3
    gpu_ids = "0,1,2,3"
    log_file = "/tmp/glm-4.7-q2k.log"

[instance.glm-4-7-q2k.server]
    # Path inside container (mounted via stage-2 mount in user_config.yml)
    # For sharded GGUF, point to the first shard.
    model = "/llm-models/GLM-4.7-GGUF/Q2_K/GLM-4.7-Q2_K-00001-of-00003.gguf"
    port = 8080
    alias = "glm4"
    # Default token budget (can still be overridden per-request).
    n_predict = 2048
    # GPU offload (equivalent to -ngl -1). Adjust for your VRAM.
    n_gpu_layers = -1

# Note:
# - GLM-4.7 prompt/template and stop tokens are typically handled client-side (per request),
#   not via this server config.
