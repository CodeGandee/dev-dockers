services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-service
    runtime: nvidia
    restart: unless-stopped
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      # Shared memory setting for performance
      - VLLM_NCCL_SO_PATH=/usr/lib/x86_64-linux-gnu/libnccl.so.2
    volumes:
      - ${HOST_VOLUME_HF_CACHE:-${HOME}/.cache/huggingface}:${CONTAINER_VOLUME_HF_CACHE:-/root/.cache/huggingface}
    ports:
      - "${HOST_PORT_API:-8000}:${CONTAINER_PORT_API:-8000}"
    ipc: host
    command: >
      --model ${MODEL_NAME:-Qwen/Qwen2-VL-7B-Instruct}
      --served-model-name ${SERVED_MODEL_NAME:-qwen2-vl}
      --trust-remote-code
      --host 0.0.0.0
      --port ${CONTAINER_PORT_API:-8000}
      --max-model-len ${MAX_MODEL_LEN:-8192}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.95}
      --enable-mm-embeds
      --allowed-local-media-path /root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-1}
              capabilities: [gpu]
